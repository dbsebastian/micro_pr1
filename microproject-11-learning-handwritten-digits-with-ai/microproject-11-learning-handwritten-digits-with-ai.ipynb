{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "cf5088aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "<div style=\"color: #DD3403; font-size: 60%\">Data Science DISCOVERY MicroProject</div>\n",
    "<span style=\"\">MicroProject #11: Learning Handwritten Digits with AI (MNIST Dataset)</span>\n",
    "<div style=\"font-size: 60%;\"><a href=\"https://discovery.cs.illinois.edu/microproject/learning-handwritten-digits-with-ai/\">https://discovery.cs.illinois.edu/microproject/learning-handwritten-digits-with-ai/</a></div>\n",
    "</h1>\n",
    "\n",
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "99b3dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Source: MNIST Database of Handwritten Digits\n",
    "\n",
    "The National Institute of Standards and Technology (NIST), an institute run by the government of the United States, provides a collection of \"Special Databases\" that *\"contain digital data objects such as images, software, and videos\" and can be found at [https://www.nist.gov/srd/related-data-products-and-links/special-databases-and-special-software](https://www.nist.gov/srd/related-data-products-and-links/special-databases-and-special-software).*  Two specific early \"Special Databases\" have become some of the most famous datasets in machine learning and artificial intelligence:\n",
    "- SD-3 contains handwriting samples from 2,100 United States census workers during the 1990 census.\n",
    "- SD-7 contains handwriting samples from 500 high school students in Maryland.\n",
    "\n",
    "As part of the 1992 paper [\"Comparison of classifier methods: a case study in handwritten digit recognition\" by LÃ©on Bottou et al.](https://ieeexplore.ieee.org/document/291440), the authors describe the creation of the \"Modified NIST\" (MNIST) dataset that:\n",
    "- Included samples from 500 unique writers (250 from each NIST dataset), and\n",
    "- Normalized all images to 28Ã—28 pixel gray scale images\n",
    "\n",
    "This MNIST dataset is now one of the **most famous datasets in machine learning and artificial intelligence**.  This dataset contains a collection of 70,000 images of hand-written digits and each are labeled with the number (ex: a picture of a \"0\" and labeled as `0`). In this MicroProject, you'll explore the MNIST dataset and learn about various clustering algorithms to help a computer learn to recognize handwritten digits.  Let's nerd out! ðŸŽ‰\n",
    "\n",
    "\n",
    "### Background Knowledge\n",
    "\n",
    "To finish this MicroProject, we assume you already know:\n",
    "\n",
    "- All topics covered in *DISCOVERY Module 1: Basics of Data Science with Python* ([review the module here](https://discovery.cs.illinois.edu/learn/))\n",
    "- *Machine Learning Models in Python with sk-learn* and *Clustering* sections of *DISCOVERY Module 6: Towards Machine Learning* ([review the module here](https://discovery.cs.illinois.edu/learn/))\n",
    "\n",
    "Let's get started! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "867107f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "e0eaeef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1: Loading the MNIST Dataset\n",
    "\n",
    "As a classic dataset, the MNIST dataset is available online in many places and many formats.  To focus on machine learning and building on the skills you already have, we have already **transformed the [ARFF file provided found on OpenML](https://www.openml.org/search?type=data&sort=runs&id=554&status=active) into a CSV file** and provided this CSV file as part of this MicroProject.\n",
    "\n",
    "Since this is a large dataset (over 120 MiB uncompressed), the file is provided as a zip file -- that won't cause any problems as pandas can read compressed files that contain a CSV inside using `pd.read_csv`.\n",
    "\n",
    "Using `pd.read_csv(...)`, load the `mnist_784.csv.xz` file into a DataFrame in the Python variable `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d8e2a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "72b2706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1.1: Viewing a Subset of Pixel Columns\n",
    "\n",
    "The first column in our DataFrame -- `label` -- is the digit represented by the following 784 pixels (`pixel1` - `pixel784`).  Each pixel column has a value from `0` to `255` representing 256 different grayscale values.  You can see a lot of variety in the pixel grayscale values near the center of the image, ex: viewing `pixel300`, `pixel301`, `pixel302`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "f22a554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the column` label` and `pixel300`-`pixel302`:\n",
    "df[ [\"label\", \"pixel300\", \"pixel301\", \"pixel302\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "1a146b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1.2: Creating the a List of All Pixel Column Names\n",
    "\n",
    "In order to build our model using only pixel data, we need to capture a list containing **only the pixel columns** (and not including the index, label, or any other columns we might add to our dataset).  To list all of the columns in the dataset, we can use `df.columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "74b0b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "76e5259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "When we have a list of elements, we can exclude elements by taking the subset **range** of a set of elements.  The syntax to get a subset of a list requires a **start index** of the subset range and the **index to stop at (and NOT include)**:\n",
    "\n",
    "As an example, if we consider this list of the five vowels:\n",
    "\n",
    "```py\n",
    "vowels = [\"A\", \"E\", \"I\", \"O\", \"U\"]\n",
    "# index:  [0]  [1]  [2]  [3]  [4]\n",
    "```\n",
    "\n",
    "The format of the subset range for the list of vowels will be `vowels[startIndex:stopIndex]`.  For example:\n",
    "\n",
    "> ```py\n",
    "> vowels[1:3]   # Starts at index [1], stops BEFORE reaching index [3]\n",
    "> # Output: [\"E\", \"I\"]           #  (...includes only [1] and [2])\n",
    "> ```\n",
    "> \n",
    "> ```py\n",
    "> vowels[0:3]   # ...start at index [0], still stopping BEFORE reaching index [3]\n",
    "> # Output: [\"A\", \"E\", \"I\"]\n",
    "> ```\n",
    ">\n",
    "> ```py\n",
    "> vowels[1:4]\n",
    "> # Output: [\"E\", \"I\", \"O\"]\n",
    "> ```\n",
    "\n",
    "Either (or both) of the starting index or the stopping index can be excluded.  If a starting index is excluded, it is understood by Python to mean \"starting from the beginning of the list\"; if the ending index is excluded, it is understood by Python to mean \"stopping only after the end of the list\".  For example:\n",
    "\n",
    "\n",
    "> ```py\n",
    "> vowels[1:]   # Starts at index [1], going until the end of the list\n",
    "> # Output: [\"E\", \"I\", \"O\", \"U\"]\n",
    "> ```\n",
    "> \n",
    "> ```py\n",
    "> vowels[:3]   # Starting at the beginning of the list, stopping BEFORE reaching index [3]\n",
    "> # Output: [\"A\", \"E\", \"I\"]\n",
    "> ```\n",
    ">\n",
    "> ```py\n",
    "> vowels[:]    # Starting at the beginning of the list and going all the way to the end (i.e.: the whole list)\n",
    "> # Output: [\"A\", \"E\", \"I\", \"O\", \"U\"]\n",
    "> ```\n",
    "\n",
    "Using this idea, create a list of **all column names that contain data about pixels** (i.e. every column except the `label` column) -- you will want to use `df.columns` from the previous code cell and select only a subset of all the columns.\n",
    "\n",
    "Store this list of all the pixels in a new Python variable named `PIXEL_COLUMNS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "6b751aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_COLUMNS = ...\n",
    "PIXEL_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "f258739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 1.2: Creating the a List of All Pixel Column Names\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"PIXEL_COLUMNS\" in vars()), \"Make sure you have defined a Python variable called `PIXEL_COLUMNS`.\"\n",
    "assert(len(PIXEL_COLUMNS) == 784), \"The variable `PIXEL_COLUMNS` must be a list containing exactly 784 elements.\"\n",
    "for i in range(1, 785): assert( f\"pixel{i}\" in PIXEL_COLUMNS ), f\"The column named `pixel{i}` must be included in PIXEL_COLUMNS.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "b5ad82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1.3: Rendering the Data\n",
    "\n",
    "Using the DataFrame `df` you loaded the MNIST dataset into and the `PIXEL_COLUMNS` list of pixel columns, we have provided an already completed function that render any index (or list of indexes) in the DataFrame.  Run the following code to define the `renderDigit` function we provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "77e7a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `renderDigit` function will render a digit from the MNIST at a specified index,\n",
    "# using the DataFrame format provided in this MicroProject:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def renderDigit(indexOrIndexList, ax = plt):\n",
    "  # If we're provided a non-list, just render it as an index:\n",
    "  if type(indexOrIndexList) is not list:\n",
    "    index = indexOrIndexList\n",
    "\n",
    "    # Select the row as the index provided and covert it into a square image.\n",
    "    #   pixel1 => (0, 0)\n",
    "    #   pixel2 => (0, 1)\n",
    "    #   pixel3 => (0, 2)\n",
    "    #   ...\n",
    "    #   pixel784 => (28, 28)\n",
    "    row = df.loc[index]\n",
    "    imgData = row[PIXEL_COLUMNS].to_numpy().reshape((28, 28))\n",
    "\n",
    "    # As a square 28x28 list of grayscale intensities, we'll make a\n",
    "    # data visualization of it so you can view it in a notebook:\n",
    "    ax.imshow(imgData, cmap=\"gray\")\n",
    "    if ax != plt:\n",
    "      ax.title.set_text(f'[{index}]')\n",
    "      ax.set_xlabel(f'\"{df.loc[index][\"label\"]}\"')\n",
    "      ax.set_xticks([])\n",
    "      ax.set_yticks([])\n",
    "    else:\n",
    "      plt.title(f'Index: {index}; Label: \"{df.loc[index][\"label\"]}\"')\n",
    "\n",
    "  # ...otherwise, it's a list and should be rendered together:\n",
    "  else:\n",
    "    fig, axes = plt.subplots(1, len(indexOrIndexList))\n",
    "    for i in range(len(indexOrIndexList)):\n",
    "      index = indexOrIndexList[i]\n",
    "      renderDigit(index, axes[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "2036e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that we have the `renderDigit` function, let's render and explore some data by running the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "dba9027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderDigit(107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "11e2d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderDigit(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "6a68daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderDigit(10707)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "b5d4b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are also some **really challenging** digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d96bb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderDigit(340)  # Is this a \"3\" or a \"7\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "bd7b5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderDigit(17772)  # Is this a \"1\" or a \"7\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "0d3eeaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "e25e6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2: Splitting the Data into the Training and Test Datasets\n",
    "\n",
    "When working with any AI algorithm, it's critical to reserve a portion of the data for **testing the model** that we're going to train. In this way we both avoid \"over-fitting\" our algorithm to specific data and have a reliable way to test if our model works with data that has never been seen before.\n",
    "\n",
    "In the MNIST dataset, the first 60,000 rows of data are used for the **training dataset** and the final 10,000 rows are used for the **testing dataset**.  In the following cell, create two DataFrames (named `train` and `test`) that contain the training and testing portion of the entire MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "81387129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the training data:\n",
    "train = ...\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "57cb5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the testing data:\n",
    "test = ...\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a1054291",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 2: Testing/Training Split\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"train\" in vars()), \"Make sure you have defined a Python variable named `train`.\"\n",
    "assert(\"test\" in vars()), \"Make sure you have defined a Python variable named `test`.\"\n",
    "assert(len(train) == 60000), \"The `train` DataFrame must have exactly 60,000 rows.\"\n",
    "assert(train.index.min() == 0), \"The first index in the `train` DataFrame must be index [0].\"\n",
    "assert(train.index.max() == 59999), \"The last index in the `train` DataFrame must be index [59999].\"\n",
    "assert(len(test) == 10000), \"The `test` DataFrame must have exactly 10,000 rows.\"\n",
    "assert(test.index.min() == 60000), \"The first index in the `test` DataFrame must be index [60000].\"\n",
    "assert(test.index.max() == 69999), \"The last index in the `test` DataFrame must be index [69999].\"\n",
    "train = train.copy()\n",
    "test = test.copy()\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "07713cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "166a556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3: Using K-Means Clustering\n",
    "\n",
    "We know from this MicroProject that the MNIST training dataset contains 60,000 labeled images, each containing the numbers 0 through 9 (ten total possible digits).\n",
    "\n",
    "### Part 3.1: Initialize the K-Means Clustering Model\n",
    "\n",
    "One of the simplest machine learning algorithms is k-means clustering.  First, create a `KMeans` clustering model stored in the Python variable `model` with a total of 10 different clusters (one cluster for each of the ten possible digits).\n",
    "\n",
    "- Not sure how to do this? [Review creating a KMeans model on the DISCOVERY website.](https://discovery.cs.illinois.edu/learn/Towards-Machine-Learning/Clustering/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "cc5ea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = ...\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "cdd48231",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.2: Training your `KMeans` model with Training Data\n",
    "\n",
    "Using the 784 columns you identified as the `PIXEL_COLUMNS` earlier in this MicroProject as your independent variable, train your `KMeans` model on your entire `train` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a02c251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "069738b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.3: Labeling Training Data with Clusters\n",
    "\n",
    "Since k-means is an **unsupervised** algorithm, we have found ten distinct clusters but we don't know what's in each cluster.  To find the best label for each cluster, we have to find the label that is most frequent in each cluster.\n",
    "\n",
    "To do this, we first need to **identify the cluster that each of training data rows exist in**.  To do this, add a new column to your DataFrame called `cluster` that contains the cluster that each row of your `train` DataFrame is in.  *(Hint: You can do this by assigning the result of `model.predict( train[PIXEL_COLUMNS] )` to the new column.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "635d3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"cluster\"] = ...\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "f843f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 3.3: Training and Labeling your KMeans model with Training Data\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"model\" in vars()), \"Make sure you have defined a Python variable called `model`.\"\n",
    "assert( model.n_clusters == 10 ), \"Your model must have 10 clusters (one for each digit), this a parameter you must include in the model initialization.\"\n",
    "assert( model.cluster_centers_.mean() > 0 ), \"You must fit your model with training data.\"\n",
    "assert(\"cluster\" in train), \"Your `train` DataFrame must contain a column named `cluster`.\"\n",
    "for i in range(10): assert( i in train[\"cluster\"] ), f\"Your `train` data must contain data labeled as cluster {i} in the `cluster` column.  (Your data never has cluster {i} appearing in the DataFrame.)\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "4bd27530",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.4: Identifying Each Cluster's Label\n",
    "\n",
    "Now we can consider any example cluster, and we'll start with `4`.  Create a DataFrame named `train_cluster4` that contains all of the training data that is labeled to be in cluster `4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "3b3fbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cluster4 = ...\n",
    "train_cluster4"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "0046745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The following code randomly chooses eight of the rows in `train_cluster4` and renders them, resulting in eight sample images from within that cluster.  If KMeans is a good clustering algorithm, all of the images **should be the same digit almost every time** since they're all in the same cluster.  Run the code below to see how the clustering did:\n",
    "\n",
    "*(You can run this code multiple times to view different samples.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "8827cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderDigit( train_cluster4.sample(n=8).index.tolist() )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "9d2e6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "Even though it's not perfect, the majority of rows in a particular cluster should have the same label (the true numerical digit for that row) -- the most popular label in each cluster can be chosen as the label for the entire cluster.  For example, here's the distribution of the labels within `train_cluster4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "2211ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cluster4[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "6e7c0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "The most common label for all the training images in `train_cluster4` value appears first and is known as the \"mode\" of the label category. It will be the \"label\" of the entire cluster.  We can generalize this approach to find the mode for every cluster by grouping by the \"cluster\" and finding the mode of the \"label\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "46307a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_label = train[[\"cluster\", \"label\"]].groupby(\"cluster\").agg(pd.Series.mode)\n",
    "cluster_label"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "52b883ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, we can use this set of cluster labels to label each cluster with the predicted label.\n",
    "\n",
    "The final code we provide is a function that uses the `cluster` value in each row and finds the corresponding `label` from the `cluster_label` DataFrame that we just found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "11bed25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelCluster(row):\n",
    "  # Find the data stored in the column \"cluster\":\n",
    "  cluster = row[\"cluster\"]\n",
    "\n",
    "  # Use the cluster to find the row in the cluster_label that contains that \"cluster\":\n",
    "  label = cluster_label.loc[cluster][\"label\"]\n",
    "\n",
    "  # Return the label we found:\n",
    "  return label"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "592f4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use `train.apply(labelCluster, axis=1)` to run the `labelCluster` function for each row in the DataFrame.  Add the result of this function to the `train` DataFrame as a new column called `predicted`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "52cbd38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"predicted\"] = ...\n",
    "train[ [\"label\", \"predicted\"] ]   # Show only the label and predicted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "49963cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 3.4: Identifying Each Cluster's Label\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"train_cluster4\" in vars()), \"Make sure you have defined a Python variable called `train_cluster4`.\"\n",
    "assert( len(train_cluster4.cluster.unique()) == 1 ), \"The data in `train_cluster4` must only be from cluster 4.\"\n",
    "assert( train_cluster4.cluster.max() == 4 ), \"The data in `train_cluster4` must only be from cluster 4.\"\n",
    "assert(\"cluster_label\" in vars()), \"Make sure you have defined a Python variable called `cluster_label`.\"\n",
    "assert(\"labelCluster\" in vars()), \"Make sure you have defined a Python function called `labelCluster`.\"\n",
    "assert(\"predicted\" in train), \"Your `train` DataFrame must contain a column named `predicted`.\"\n",
    "for i in range(10):\n",
    "  assert( len(train[train.cluster == i][\"predicted\"].unique()) == 1 ), f\"The predicted label value for all data in cluster {i} must be the same (all the rows are in the same cluster).\"\n",
    "  assert( train[train.cluster == i][\"predicted\"].max() == cluster_label.loc[i][\"label\"] ), f\"The predicted label value for all data in cluster {i} must be {cluster_label.loc[i]['label']}.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "d4ebedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.5: Label the Testing Data with a Prediction\n",
    "\n",
    "In the previous code, we labeled the **training data** with the predicted label.  However, the **testing data** is data that our model has never seen before and should be used to find the accuracy of our model.\n",
    "\n",
    "First, using `model`, add a new column `\"cluster\"` to the `test` DataFrame.  *(This will be very similar to adding it to the `train` DataFrame, but now we're using a different DataFrame but the same trained model.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "32edf5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each row in the `test` DataFrame with its \"cluster\":\n",
    "test[\"cluster\"] = ...\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "160fe88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, apply the `labelCluster` function to every row in your `test` DataFrame and add a new column called `predicted` to your `test` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "2313ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each row in the `test` DataFrame with its \"predicted\" label:\n",
    "test[\"predicted\"] = ...\n",
    "test[[\"label\", \"predicted\"]]    # Show only the label and predicted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "7915f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 3.5: Label the Testing Data with a Prediction\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"cluster\" in test), \"Your `test` DataFrame must contain a column named `cluster`.\"\n",
    "assert(\"predicted\" in test), \"Your `test` DataFrame must contain a column named `predicted`.\"\n",
    "for i in range(10):\n",
    "  assert( len(test[test.cluster == i][\"predicted\"].unique()) == 1 ), f\"The predicted label value for all data in cluster {i} must be the same (all the rows are in the same cluster).\"\n",
    "  assert( test[test.cluster == i][\"predicted\"].max() == cluster_label.loc[i][\"label\"] ), f\"The predicted label value for all data in cluster {i} must be {cluster_label.loc[i]['label']}.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "94687726",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.6: Calculate the Prediction Error\n",
    "\n",
    "The correct prediction for each row in our DataFrame is when the **`label` matches the `predicted` label**.  A reliable algorithm should have a very low error rate, and any error rate above 5% will be quickly noticed by humans.\n",
    "\n",
    "Find the number of rows where the `label` does not match the `predicted` label in the `test` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "bd164148",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predictions = ...\n",
    "incorrect_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "f9da5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Convert the number of errors into a percentage (decimal form) of the 10,000 test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "56122827",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_error_pct = ...\n",
    "kmeans_error_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "362d07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 3.6: Calculate the Prediction Error\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"incorrect_predictions\" in vars()), \"Make sure you have defined a Python variable called `incorrect_predictions`.\"\n",
    "assert(\"kmeans_error_pct\" in vars()), \"Make sure you have defined a Python variable called `kmeans_error_pct`.\"\n",
    "assert( kmeans_error_pct > 0.3 and kmeans_error_pct < 0.6 ), \"Make sure you correctly calculated the prediction error as a percentage between 0 and 1.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "6e4f4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "9dfde94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 4: Using k-Nearest Neighbors (kNN) Classifier\n",
    "\n",
    "In Part 3, you found that clustering the features using KMeans failed to accurately predict data.  Many times in AI and machine learning, the first approach you try doesn't yield great results.  Finding the right algorithm for a specific dataset is very hard and knowing hundreds of different algorithms and when to apply them makes you a great data scientist.\n",
    "\n",
    "Let's try another simple classification algorithm -- but this time, let's see if a supervised algorithm can give us a better result!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "f186de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 4.1: Finding Neighbors\n",
    "\n",
    "The k-Nearest Neighbor algorithm is simple: find the `k` closest points from your training data and use the most commonly occurring label.  That's it!\n",
    "\n",
    "Let's start by taking the very first row in our `test` dataset and call it our `sample`:\n",
    "\n",
    "*(Hint: what is the index of the first row in the `test` dataset? How do you get a row given its index?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "05143796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first row from the `test` DataFrame:\n",
    "sample = ...\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "2bc879bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Once we have our `sample`, we can find the difference between the values in each `pixel` column by subtracting the entire training DataFrame from the sample to get a DataFrame of the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "0d592211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the squared difference between the each of the pixel column values\n",
    "# between each row in our `train` DataFrame and the sample:\n",
    "distance = (train[PIXEL_COLUMNS] - sample[PIXEL_COLUMNS]) ** 2\n",
    "\n",
    "# Take the square root of the sum of all the values:\n",
    "train[\"distance\"] = distance.sum(axis=1) ** 0.5\n",
    " \n",
    "# ...and display a center-area pixel to make sure it's working:\n",
    "train[ [\"label\", \"distance\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "e323c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the `distance` column that was just calculated, find the 5 rows in your `train` dataset that have the **smallest distance** and store those five rows in the DataFrame `neighbors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "e5528cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = ...\n",
    "neighbors[ [\"label\", \"distance\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "f002cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the above DataFrame, you should find that **all of the neighbors** of the first sample are labeled with `7`.  This means we predict that the label of our sample is `7`.  *Is that what it was label when we found our sample at the beginning of Part 4.1?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "db086aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 4.1: Finding Neighbors\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"neighbors\" in vars()), \"Make sure you have defined a Python variable called `neighbors`.\"\n",
    "assert(len(neighbors) == 5), \"Your `neighbors` DataFrame must have exactly 5 rows.\"\n",
    "assert(sum(neighbors.index) == 182711), \"The rows in your `neighbors` DataFrame are the incorrect neighbors.  (Is your sample correct?)\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "f62472b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 4.2: Training the `KNeighborsClassifier`\n",
    "\n",
    "Similar to `LinearRegression`, the `KNeighborsClassifier` in the sklearn library is a supervised machine learning algorithm.  Unlike linear regression, `KNeighborsClassifier` will predict a label (category) instead of a number (regression).\n",
    "\n",
    "First, initialize an instance of the `KNeighborsClassifier` using all the default parameters (you don't need to specify anything in the initialization) as `model2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "08e213d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "2df6fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second, identify the independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "72022eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the independent variables used in our prediction:\n",
    "ind = ...\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "4a2beb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the dependent variable in our `train` DataFrame (what column contains the label)?\n",
    "dep = ...\n",
    "dep"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "2a0b8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, fit the `train` data to our `KNeighborsClassifier` that we called `model2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "c5b13c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(train[ind], train[dep])\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "b37a76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 4.3: Understanding the `KNeighborsClassifier` Neighbors\n",
    "\n",
    "First, let's refresh what neighbors we found for the first row in our test DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "011cb8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the \"distance\" column for the `neighbors` DataFrame:\n",
    "neighbors[ [\"distance\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "d6f3dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Before we make a prediction, let's check if the neighbors we found ourselves matches the ones found by the `KNeighborsClassifier` algorithm below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "21d33934",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.kneighbors( test[:1][PIXEL_COLUMNS] )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "6442fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "If everything is correct so far, the distances (first list) and the indexes (second list) in the cell above should exactly match the DataFrame you created earlier. ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "4fc858eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 4.4: Labeling Predictions Using the `KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "e092c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's first make a prediction using our sample, and see if our prediction matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "5132e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict( test[:1][PIXEL_COLUMNS] )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "e77a9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, add a new `predicted` column to the entire `test` dataset by predicting the value of the **full** test dataset by using your `model2` classifier:\n",
    "\n",
    "**(NOTE: This may take up to minute on a slower laptop, there's A LOT of distances to find!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "8bd5bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"predicted\"] = ...\n",
    "test[ [\"label\", \"predicted\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a25bdcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 4.4: Labeling Predictions Using the KNeighborsClassifier\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"model2\" in vars()), \"Make sure you have defined a Python variable called `model2`.\"\n",
    "assert(model2.n_features_in_ == 784), \"Your `model2` must have 784 independent variables (each pixel value).\"\n",
    "assert((\"predict\" in test and sum(test.label != test.predict) == 312) or sum(test.label != test.predicted) == 312), \"Your `test` predictions are not a prediction from a kNN model.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "37d81abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 4.5: Calculate Prediction Error\n",
    "\n",
    "Just like with KMeans clustering, the correct prediction for each row in our DataFrame is when the **`label` matches the `predicted` label**.  A reliable algorithm should have a very low error rate, and any error rate above 5% will be quickly noticed by humans.\n",
    "\n",
    "Now that we labeled our `test` DataFrame using kNN, find the number or rows where the `label` does not match the `predicted` label in the `test` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "cf672a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predictions = ...\n",
    "incorrect_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "bea1089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Just like before, convert this to a percentage (in decimal form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "c1277e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_error_pct = ...\n",
    "knn_error_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "5c0c7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 4.5: Calculate Prediction Error\n",
    "import math\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"knn_error_pct\" in vars()), \"Make sure you have defined a Python variable called `knn_error_pct`.\"\n",
    "assert(\n",
    "  math.isclose(knn_error_pct, sum(test.label != test.predicted) * 1e-4) or \n",
    "  (\"predict\" in test and math.isclose(knn_error_pct, sum(test.label != test.predict) * 1e-4))\n",
    "), \"Your `knn_error_pct` value is incorrect.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "f4f13788",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 4.6: Comparing Error\n",
    "\n",
    "Running the following cell will provide a summary of the two error rates you calculated on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a1a5c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "  {\"Model\": \"K-Means Clustering\", \"MNIST Test Suite Prediction Error\": kmeans_error_pct},\n",
    "  {\"Model\": \"K-Nearest Neighbors\", \"MNIST Test Suite Prediction Error\": knn_error_pct},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "6c7e48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exploring how different models perform with different datasets, and understanding why a model works and doesn't work, is one of the most foundational things in Data Science:\n",
    "- Which model performed better on the MNIST test data?\n",
    "- Was there a significant difference in the error rates?\n",
    "\n",
    "You can now use your trained model to predict writing you have never seen -- and recognize any handwritten digit! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "a971f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "id": "ebe7e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission\n",
    "\n",
    "You're almost done!  All you need to do is to commit your lab to GitHub and run the GitHub Actions Grader:\n",
    "\n",
    "1.  âš ï¸ **Make certain to save your work.** âš ï¸ To do this, go to **File => Save All**\n",
    "\n",
    "2.  After you have saved, exit this notebook and return to https://discovery.cs.illinois.edu/microproject/learning-handwritten-digits-with-ai/ and complete the section **\"Commit and Grade Your Notebook\"**.\n",
    "\n",
    "3. If you see a 100% grade result on your GitHub Action, you've completed this MicroProject! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
