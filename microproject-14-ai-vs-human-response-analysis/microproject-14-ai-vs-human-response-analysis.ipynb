{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "<div style=\"color: #DD3403; font-size: 60%\">Data Science DISCOVERY MicroProject</div>\n",
    "<span style=\"\">MicroProject #14: AI vs Human: Response Analysis</span>\n",
    "<div style=\"font-size: 60%;\"><a href=\"https://discovery.cs.illinois.edu/microproject/ai-vs-human-response-analysis/\">https://discovery.cs.illinois.edu/microproject/ai-vs-human-response-analysis/</a></div>\n",
    "</h1>\n",
    "\n",
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Source: Hugging Face HC3 Dataset\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) is a company that has developed a platform for natural language processing (NLP) applications. They have created and shared a large collection of pre-trained models, datasets, and learning resources which are open-source and available for the public to use.\n",
    "\n",
    "Hello-SimpleAI is a small group of researchers (PhD students and engineers) that released the [HC3 dataset](https://arxiv.org/abs/2301.07597). This dataset compares human responses (from Reddit, Wikipedia, and other sources) and AI responses to the same question, with details available on their GitHub repository ([@Hello-SimpleAI/chatgpt-comparison-detection](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)) or via the [Hugging Face API](https://huggingface.co/datasets/Hello-SimpleAI/HC3/viewer/all/train).\n",
    "\n",
    "Are responses from AI different from humans? Let's nerd out with with this data and find out! ðŸŽ‰\n",
    "\n",
    "\n",
    "### Background Knowledge\n",
    "\n",
    "To finish this MicroProject, we assume you already know:\n",
    "\n",
    "- All topics covered in *DISCOVERY Module 1: Basics of Data Science with Python* ([review the module here](https://discovery.cs.illinois.edu/learn/))\n",
    "- Adding new columns into an existing DataFrame ([review creating new columns here](https://discovery.cs.illinois.edu/guides/Modifying-DataFrames/adding-columns-in-dataframes/))\n",
    "- Working with functions in Python ([review Python functions here](https://discovery.cs.illinois.edu/learn/Simulation-and-Distributions/Functions-in-Python/))\n",
    "- Understanding (and performing) hypothesis tests ([review hypothesis testing here](https://discovery.cs.illinois.edu/learn/Polling-Confidence-Intervals-and-Hypothesis-Testing/Hypothesis-Testing/))\n",
    "\n",
    "Let's get started! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1: Loading the Dataset\n",
    "\n",
    "For this MicroProject, we have provided the first 100 samples of the `Hello-SampleAI` dataset that was gathered from the following URL: https://datasets-server.huggingface.co/rows?dataset=Hello-SimpleAI%2FHC3&config=all&split=train&offset=0&length=100\n",
    "\n",
    "This data has been **cleaned up and provided to you** as `huggingface-hello-sampleAI-100.csv`.  This dataset has 100 `question`s that were asked and answered on the  \"Explain Like I'm Five\" (ELI5) subreddit, with the `human_answer` from reddit and a `chatgpt_answer` from ChatGPT.\n",
    "\n",
    "Load the cleaned up CSV file dataset as a DataFrame `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### View Sample Outputs\n",
    "\n",
    "Run the following cell to view a random question and the human and AI answers to get a feeling for this data (run it again to view a different random question).\n",
    "- *Note that this text has already been tokenized, an early step in almost all text analysis that breaks a sentence into the small tokens for further processing, and may have extra spaces not naturally seen outside of NLP.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to view a random question and the Human and AI answers to\n",
    "# get a feeling for this data (run it again to view a different random question).\n",
    "sample = df.sample(n=1)\n",
    "print(\"== Question ==\")\n",
    "print(f\"{sample.question.values[0]}\")\n",
    "print()\n",
    "print(\"== Human Answer (reddit response) ==\")\n",
    "print(f\"{sample.human_answer.values[0]}\")\n",
    "print()\n",
    "print(\"== AI Answer (ChatGPT) ==\")\n",
    "print(f\"{sample.chatgpt_answer.values[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 1: Loading the Dataset\n",
    "# - This read-only cell contains a \"checkpoint\" for this section of the MicroProejct and verifies you are on the right track.\n",
    "# - If this cell results in a celebration message, you PASSED all test cases!\n",
    "# - If this cell results in any errors, check you previous cells, make changes, and RE-RUN your code and then this cell.\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "df = df.replace(r'\\r\\n', '\\n', regex=True)\n",
    "assert(\"df\" in vars()), \"Make sure your data is stored in `df`.\"\n",
    "assert(len(df) == 100), \"Your `df` is not the correct length.\"\n",
    "assert(\"How come I have to pay for Foxtel and get ads , when things like youtube are free because they play ads . Should n't Foxtel be you pay and you do nt get ads , or it should be free and you get ads . Please explain like I'm five.\" in df[\"question\"].values)\n",
    "assert(\"It's important to remember that Japan is a country with a rich and diverse culture, and like any other country, it has a range of cultural practices and expressions. While it is true that Japan has a reputation for being a more traditional and conservative culture, it is also home to a vibrant and creative media industry. \\nThe Japanese media industry is known for producing a wide variety of content, including video games, TV shows, and music, that often incorporates elements of Japanese culture and history. This can include things like traditional Japanese instruments and themes, as well as more modern and unconventional elements. \\nIt's also worth noting that different parts of Japanese culture can have different levels of conservatism. For example, some aspects of Japanese culture, such as traditional family structure or gender roles, may be more conservative, while other parts of Japanese culture, such as the media industry, may be more open to experimentation and innovation. \\nOverall, it's important to remember that Japan is a complex and multifaceted culture, and it's not fair to generalize or stereotype all aspects of Japanese culture based on a few specific examples.\" in df[\"chatgpt_answer\"].values)\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2: Response Length Analysis\n",
    "\n",
    "This MicroProject attempts to identify features that may be distinctive between a human response and an AI response.  One common critique of current AI systems are that the answers are often excessively long.  Our first hypothesis to investigate is that an AI response may be longer than a human response.\n",
    "\n",
    "To explore this, we will explore the average length of an AI response and see if there's any statistical significance to the difference from the human response."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2.1: Finding the Response Length"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform the analysis of response length, we first need to two new columns in our DataFrame for each of our 100 sample questions.  When working with columns that contain `string` data, we work with the individual strings as data using the following **generic syntax**:\n",
    "\n",
    "> ```py\n",
    "> df[\"column\"].str.STRING_FUNCTION()\n",
    "> ```\n",
    "\n",
    "...where `STRING_FUNCTION()` is any of the [string accessor methods](https://pandas.pydata.org/docs/reference/series.html#api-series-str).\n",
    "\n",
    "Specifically, one string accessor method is:\n",
    "\n",
    "> `[...].str.len()`: Compute the length of each element in the Series/Index.\n",
    "\n",
    "To begin our analysis of the length of each response, add two new columns to the DataFrame:\n",
    "- `human_answer_len`, containing the length (in characters) of the human response\n",
    "- `chatgpt_answer_len`, containing the length (in characters) of the ChatGPT response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2.2: Compute the Average Answer Lengths\n",
    "\n",
    "Now that we have the lengths of both responses, save the average length of the responses in the variables `human_avg` and `ai_avg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average response length for a human response:\n",
    "human_avg = ...\n",
    "human_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average response length for a ChatGPT response:\n",
    "ai_avg = ...\n",
    "ai_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 2.2: Compute the Average Answer Lengths\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"human_avg\" in vars()), \"Your average response length for the human responses has to be stored in the variable `human_avg`.\"\n",
    "assert(\"ai_avg\" in vars()), \"Your average response length for the ai responses has to be stored in the variable `ai_avg`.\"\n",
    "assert('human_answer_len' in df.columns), \"The lengths of human responses should be in a column named `human_answer_len` in `df`.\"\n",
    "assert('chatgpt_answer_len' in df.columns), \"The lengths of ChatGPT responses should be in a column named `chatgpt_answer_len` in `df`.\"\n",
    "assert(876 in df[\"human_answer_len\"].values and 930 in df[\"chatgpt_answer_len\"].values), \"Make sure you're using the correct string accessor method.\"\n",
    "assert((human_avg * 390)/30 == 9488.44), \"`human_avg` was calculated incorrectly.\"\n",
    "assert(((ai_avg * 444)/30) + 86 == 16347.796), \"`ai_avg` was calculated incorrectly.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2.3: Testing if the Difference in Length is Statistically Significant\n",
    "\n",
    "To determine if the difference between the means is statistically significant, we're going to conduct a t-test. Let's first state our null and alternative hypotheses.\n",
    "\n",
    "> **Null hypothesis**: There is no significant difference between the means of the two response lengths.\n",
    "> \n",
    "> **Alternative hypothesis**: There is a significant difference between the means of the two response lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the [`scipy.stats.ttest_ind()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) function to get the t-stat (as `t_stat`) and p-value (as `p_value`).\n",
    "\n",
    "You can find the function documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a t-test:\n",
    "#    Syntax: t_stat, p_value = scipy.stats.ttest_ind( ... )\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the t-statistic:\n",
    "t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the p-value:\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2.4: Choosing Your Conclusion\n",
    "\n",
    "Using the calculated p-value and an alpha level of 0.05, we can determine if there is a significant difference between the two mean values.\n",
    "\n",
    "For each set of comments, un-comment **exactly one line in each set** corresponding to the true statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Un-comment the correct statement for the p-value: ==\n",
    "#conclusion_p_value = \"The p-value is greater than 0.05 (5%)\"\n",
    "#conclusion_p_value = \"The p-value is less than 0.05 (5%)\"\n",
    "\n",
    "# == Un-comment the correct statement for the significance: ==\n",
    "#conclusion_significance = \"there IS a statistically significant difference\"\n",
    "#conclusion_significance = \"there is NOT a statistically significant difference\"\n",
    "\n",
    "# == We will print your conclusion ==\n",
    "print(f\"Conclusion: {conclusion_p_value} so {conclusion_significance} difference between the means of the response lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 2.4: Response Length Analysis\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"t_stat\" in vars()), \"Store your t-statistic in the variable `t_stat`. Check the ttest_ind() documentation or the comment in that code box for the correct syntax.\"\n",
    "assert(\"p_value\" in vars()), \"Store your p-value in the variable `p_value`. Check the ttest_ind() documentation or the comment in that code box for the correct syntax.\"\n",
    "\n",
    "import math\n",
    "assert( math.isclose(t_stat * p_value, -0.005055524973109543) ), \"Check your t_stat and p_value.\"\n",
    "assert( len(conclusion_p_value) * len(conclusion_significance) == 1598 ), \"Your conclusion is incorrect.\"\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3: Sentiment Analysis\n",
    "\n",
    "A second common critique of current AI systems are the answers are often **more subjective** and **optimistic** than a human.\n",
    "\n",
    "There is a field of Data and Computer Science called \"Natural Language Processing (NLP)\" that involves algorithms to analyze the \"polarity\" and \"subjectivity\" score of a piece of text.  In the next two parts of this MicroPorject, we will explore the polarity and subjectivity of human and AI responses."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.1: Using TextBlob to Find the Polarity Scores\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. \n",
    "\n",
    "Among the many features, TextBlob can calculate a polarity score for any given piece of text.  A \"polarity score\" ranges from -1 to 1, where:\n",
    "- A score of -1 is very negative,\n",
    "- A score of 0 is neutral, and\n",
    "- A score of 1 is very positive.\n",
    "\n",
    "For example, the text *\"You will fail DISCOVERY if you do not take the final exam\"* is a negative statement and has a polarity score of -0.25.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.2: Installing TextBlob\n",
    "\n",
    "To use the TextBlob library, we first need to start installing the library:\n",
    "\n",
    "1. In your terminal, type the following: `python3 -m pip install textblob`\n",
    "\n",
    "    - If this command does not work, try the others `pip install` commands listed [here](https://discovery.cs.illinois.edu/guides/System-Setup/Setup-Your-System/#Step-4-Installing-pandas), replacing `pandas` with `textblob`.\n",
    "\n",
    "2. Once it is installed, import the `TextBlob` library with the following import statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.3: Creating a `polarity` Function\n",
    "\n",
    "To allow us to easily find the \"polarity score\" of hundreds of pieces of text using pandas, we need a function that will take a string `s` as input and return the polarity score.\n",
    "\n",
    "Complete the `polarity` function below, by:\n",
    "- Reading the [TextBlob Tutorial: Quickstart](https://textblob.readthedocs.io/en/dev/quickstart.html), specifically focusing on the \"Sentiment Analysis\" portion of the the tutorial.\n",
    "- Finish the `polarity` so that the function returns the polarity score of the input string `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(s):\n",
    "  ...\n",
    "  return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, let's test your function to make sure it's working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the documentation, we expect a positive (above zero) polarity:\n",
    "polarity(\"Textblob is amazingly simple to use. What great fun!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A negative statement, we expect a negative (below zero) polarity:\n",
    "polarity(\"You will fail DISCOVERY if you do not take the final exam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neutral statement, we expect a zero polarity:\n",
    "polarity(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try your own sentence :)\n",
    "polarity(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.4: Finding the Polarity of Human Answers\n",
    "\n",
    "Once you have defined a custom function, you can use it on every row of data by using:\n",
    "> ```python\n",
    "> df[\"column\"].apply( customFunctionName )\n",
    "> ```\n",
    "\n",
    "Note that `customFunctionName` is **ONLY the function name** and does NOT contain the **function call** (there are no parenthesis after the function name; it's just the name of the function). You are providing ONLY the **name of the function** and the `apply` function will internally **call** the function.\n",
    "\n",
    "Using the syntax above, create a new column named `human_polarity` that finds the polarity of the `human_answer` data using the `.apply` operation on a column of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"human_polarity\"] = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.5: Finding the Polarity of ChatGPT Answers\n",
    "\n",
    "Now, find the polarity of ChatGPT answers and save each polarity in the column `chatgpt_polarity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"chatgpt_polarity\"] = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 3: Sentiment Analysis - Polarity\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "\n",
    "assert('human_polarity' in df.columns), \"You must have add a column named `human_polarity` to your DataFrame.\"\n",
    "assert('chatgpt_polarity' in df.columns), \"You must have add a column named `chatgpt_polarity` to your DataFrame.\"\n",
    "\n",
    "import math\n",
    "assert(math.isclose( df[\"human_polarity\"].mean(), 0.07790771420489018 )), \"The calculation of the `human_polarity` is incorrect.\"\n",
    "assert(math.isclose( df[\"chatgpt_polarity\"].std(), 0.11014935757814867 )), \"The calculation of the `chatgpt_polarity` is incorrect.\"\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analysis: Overall Results So Far\n",
    "\n",
    "Run the following cell to see a summary of the results you found so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "  {\"Metric\": \"Response Length (mean):\",  \"Human\": human_avg, \"ChatGPT\": ai_avg, \"p-value\": p_value, \"significant\": (\"Yes (p < 0.05)\" if p_value < 0.05 else \"No\") },\n",
    "  {\"Metric\": \"Polarity (mean):\", \"Human\": df.human_polarity.mean(), \"ChatGPT\": df.chatgpt_polarity.mean(), \"p-value\": \"(not tested)\", \"significant\": \"?\" },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 4: Sentiment Analysis - Subjectivity\n",
    "\n",
    "TextBlob can also calculate a subjectivity score for any given piece of text.  A \"subjectivity score\" ranges from 0 to 1, where:\n",
    "- A score of 0 is not a subjective statement, and\n",
    "- A score of 1 is very subjective statement\n",
    "\n",
    "Using the same library and design as Part 2, create a function called `subjectivity` that returns the subjectivity score of a given string `s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity(s):\n",
    "  ...\n",
    "  return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A highly subjective statement, expecting a score above 0.5:\n",
    "subjectivity(\"The best fruit is a kiwi and everyone else is wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A factual statement, expecting a score near 0:\n",
    "subjectivity(\"Winter is colder than summer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try your own :)\n",
    "subjectivity(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 4.1: Finding the Subjectivity of Human and ChatGPT Answers\n",
    "\n",
    "Add two new columns to the DataFrame, `human_subjectivity` and `chatgpt_subjectivity`, that stores the subjectivity of each answer for all of the questions.\n",
    "\n",
    "*You may need to look back to the previous section to review how to use the .apply function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"human_subjectivity\"] = ...\n",
    "df[\"chatgpt_subjectivity\"] = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 4: Sentiment Analysis - Subjectivity\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "import math\n",
    "assert('human_subjectivity' in df.columns), \"You must have add a column named `human_subjectivity` to your DataFrame.\"\n",
    "assert('chatgpt_subjectivity' in df.columns), \"You must have add a column named `chatgpt_subjectivity` to your DataFrame.\"\n",
    "assert( math.isclose( df[\"human_subjectivity\"].mean(), 0.4507534813188739 ) ), \"Your calculation of `human_subjectivity` is incorrect.\"\n",
    "assert( math.isclose( df[\"chatgpt_subjectivity\"].std(), 0.10025355258208316 ) ), \"Your calculation of `chatgpt_subjectivity` is incorrect.\"\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analysis: Overall Results So Far\n",
    "\n",
    "Run the following cell to see a summary of the results you found so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "  {\"Metric\": \"Response Length (mean):\",  \"Human\": human_avg, \"ChatGPT\": ai_avg, \"p-value\": p_value, \"significant\": (\"Yes (p < 0.05)\" if p_value < 0.05 else \"No\") },\n",
    "  {\"Metric\": \"Polarity (mean):\", \"Human\": df.human_polarity.mean(), \"ChatGPT\": df.chatgpt_polarity.mean(), \"p-value\": \"(not tested)\", \"significant\": \"?\" },\n",
    "  {\"Metric\": \"Subjectivity (mean of absolutely values):\", \"Human\": df.human_subjectivity.mean(), \"ChatGPT\": df.chatgpt_subjectivity.mean(), \"p-value\": \"(not tested)\", \"significant\": \"?\" },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 5: Testing if the Difference in Subjectivity is Statistically Significant"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "To determine if the difference between the subjectivity is statistically significant, we're going to conduct another t-test. Let's first state our null and alternative hypotheses.\n",
    "\n",
    "> **Null hypothesis**: There is no significant difference between the means of the human and ChatGPT subjectivity.\n",
    "> \n",
    "> **Alternative hypothesis**: There is a significant difference between the means of the human and ChatGPT subjectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 5.1: Preforming the t-test\n",
    "\n",
    "Use the `scipy.stats.ttest_ind` function again to get the t-stat and p-value, storing it as `t_stat_subjectivity` and `p_value_subjectivity` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a t-test:\n",
    "#    Syntax: t_stat_subjectivity, p_value_subjectivity = ttest_ind( ... )\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat_subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 5.2: Choosing Your Conclusion\n",
    "\n",
    "Using the calculated p-value and an alpha level of 0.05, we can determine if there is a significant difference between the two mean values.\n",
    "\n",
    "For each set of comments, un-comment **exactly one line in each set** corresponding to the true statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Un-comment the correct statement for the p-value: ==\n",
    "#conclusion2_p_value = \"The p-value is greater than 0.05 (5%)\"\n",
    "#conclusion2_p_value = \"The p-value is less than 0.05 (5%)\"\n",
    "\n",
    "# == Un-comment the correct statement for the significance: ==\n",
    "#conclusion2_significance = \"there IS a statistically significant difference\"\n",
    "#conclusion2_significance = \"there is NOT a statistically significant difference\"\n",
    "\n",
    "# == We will print your conclusion ==\n",
    "print(f\"Conclusion: {conclusion2_p_value} so {conclusion2_significance} difference between the means of the subjectivity scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASE for Part 5: Testing if the Difference in Subjectivity is Statistically Significant\n",
    "tada = \"\\N{PARTY POPPER}\"\n",
    "assert(\"t_stat\" in vars())\n",
    "assert(\"p_value\" in vars())\n",
    "\n",
    "import math\n",
    "assert( math.isclose(t_stat_subjectivity * p_value_subjectivity, -0.031921405408979774) ), \"Check your t_stat and p_value.\"\n",
    "assert( len(conclusion2_p_value) * len(conclusion2_significance) == 1598 ), \"Your conclusion is incorrect.\"\n",
    "\n",
    "print(f\"{tada} All Tests Passed! {tada}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analysis: Results\n",
    "\n",
    "Run the following cell to see a summary of the results of all of the metrics (we finished the t-test for polarity for you as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "  {\"Metric\": \"Response Length (mean):\",  \"Human\": human_avg, \"ChatGPT\": ai_avg, \"p-value\": p_value, \"significant\": (\"Yes (p < 0.05)\" if p_value < 0.05 else \"No\") },\n",
    "  {\"Metric\": \"Polarity (mean):\", \"Human\": df.human_polarity.mean(), \"ChatGPT\": df.chatgpt_polarity.mean(), \"p-value\": 0.3258256055853501, \"significant\": \"No\" },\n",
    "  {\"Metric\": \"Subjectivity (mean of absolutely values):\", \"Human\": df.human_subjectivity.mean(), \"ChatGPT\": df.chatgpt_subjectivity.mean(), \"p-value\": p_value_subjectivity, \"significant\": (\"Yes (p < 0.05)\" if p_value_subjectivity < 0.05 else \"No\") },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr style=\"color: #DD3403;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission\n",
    "\n",
    "You're almost done!  All you need to do is to commit your lab to GitHub and run the GitHub Actions Grader:\n",
    "\n",
    "1.  âš ï¸ **Make certain to save your work.** âš ï¸ To do this, go to **File => Save All**\n",
    "\n",
    "2.  After you have saved, exit this notebook and return to https://discovery.cs.illinois.edu/microproject/ai-vs-human-response-analysis/ and complete the section **\"Commit and Grade Your Notebook\"**.\n",
    "\n",
    "3. If you see a 100% grade result on your GitHub Action, you've completed this MicroProject! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
